{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class02_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXBlEPuv5hfbxOZ4wfrGGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1antw/Application-of-AI/blob/main/Class02_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpCaPv_zKxF5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grad = False\n",
        "tensor = torch.rand(3, requires_grad=True)\n",
        "print(tensor)\n",
        "\n",
        "y = tensor + 6\n",
        "print(y)\n",
        "z = 3*y**2 + 2\n",
        "print(z)\n",
        "z = z.mean() #z = z/3\n",
        "print(z)\n",
        "z.backward() # dz/d(tensor)\n",
        "print(tensor.grad) #6(t+6)/3 -> 2*tensor+12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06CVj04bSmFb",
        "outputId": "55b3c10e-2b1c-4272-e72f-b3219dd87e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1849, 0.1943, 0.2380], requires_grad=True)\n",
            "tensor([6.1849, 6.1943, 6.2380], grad_fn=<AddBackward0>)\n",
            "tensor([116.7602, 117.1085, 118.7390], grad_fn=<AddBackward0>)\n",
            "tensor(117.5359, grad_fn=<MeanBackward0>)\n",
            "tensor([12.3699, 12.3886, 12.4761])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not scalar(會error，只能放scalar)\n",
        "'''\n",
        "tensor = torch.tensor([0.37, 0.58, 0.33], requires_grad=True)\n",
        "print(tensor)\n",
        "\n",
        "y = tensor + 6\n",
        "print(y)\n",
        "z = 3*y**2 + 2 #通常要做mean, 像在cross entropy上\n",
        "print(z)\n",
        "z.backward()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_573ma1AU2Wt",
        "outputId": "ad80f05c-c9c6-474f-cf38-4ad11839acaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntensor = torch.tensor([0.37, 0.58, 0.33], requires_grad=True)\\nprint(tensor)\\n\\ny = tensor + 6\\nprint(y)\\nz = 3*y**2 + 2\\nprint(z)\\nz.backward()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grad \n",
        "tensor = torch.rand(3, requires_grad = True)\n",
        "print(tensor)\n",
        "y = tensor.detach() #train做完截斷，否則在test時會改變(不行)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXAllKC7U8nv",
        "outputId": "c5f2f97b-c130-4f5d-df26-4b65e49c8a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7105, 0.7598, 0.4512], requires_grad=True)\n",
            "tensor([0.7105, 0.7598, 0.4512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grad \n",
        "with torch.no_grad(): #此之下torch不做微分(較常用)\n",
        "  y = tensor + 2\n",
        "  print(y)\n",
        "#圖像鎖前面動後面，語音鎖後面動前面"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXwhjkhoVeLw",
        "outputId": "87937fac-3173-4dba-d035-4cb90358fce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.7105, 2.7598, 2.4512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accumulated gradient\n",
        "weights = torch.tensor([2., 3., 5., 7.], requires_grad=True)\n",
        "\n",
        "for epoch in range(5):\n",
        "  outputs = (3*weights).sum() #wight的gradient會存起來，累加\n",
        "  outputs.backward()\n",
        "\n",
        "  print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGkDwUnOVi8q",
        "outputId": "e53bb280-b0d1-4533-cac8-b12a2fd6170f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n",
            "tensor([12., 12., 12., 12.])\n",
            "tensor([15., 15., 15., 15.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# empty gradient\n",
        "weights = torch.tensor([2., 3., 5., 7.], requires_grad=True)\n",
        "\n",
        "for epoch in range(5):\n",
        "  outputs = (3*weights).sum()\n",
        "  outputs.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  weights.grad.zero_() #清除weight的值，要記得清零"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtHDGzZjWTja",
        "outputId": "e96f34b8-ccd4-4562-f859-084ad8ad6dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example\n",
        "x = torch.tensor([[1,-1], [2,3], [5,2]], dtype=torch.float32) # 3x2\n",
        "y = torch.tensor([[1],[0],[1]], dtype=torch.float32)\n",
        "\n",
        "w1 = torch.rand(2,3, requires_grad=True) # 2x3\n",
        "w2 = torch.rand(3,3, requires_grad=True) # 3x3\n",
        "w3 = torch.rand(3,2, requires_grad=True) # 3x2\n",
        "w4 = torch.rand(2,1, requires_grad=True) # 2x1\n",
        "relu = nn.ReLU()\n",
        "sigmoid = nn.Sigmoid()\n",
        "bce = nn.BCELoss()\n",
        "\n",
        "def forward(inputs):\n",
        "    inputs = torch.matmul(inputs, w1) # matrix multiply\n",
        "    inputs = relu(inputs)\n",
        "    inputs = torch.matmul(inputs, w2)\n",
        "    inputs = relu(inputs)\n",
        "    inputs = torch.matmul(inputs, w3)\n",
        "    inputs = relu(inputs)\n",
        "    inputs = torch.matmul(inputs, w4)\n",
        "    outputs = sigmoid(inputs)\n",
        "    return outputs\n",
        "\n",
        "# loss\n",
        "def loss(y_true, y_pred):\n",
        "    return bce(y_pred, y_true)\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # forward pass\n",
        "    y_hat = forward(inputs=x)\n",
        "\n",
        "    # loss\n",
        "    bce_loss = loss(y_true=y, y_pred=y_hat)\n",
        "\n",
        "    # backward loss\n",
        "    bce_loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "    w3 -= learning_rate * w3.grad\n",
        "    w4 -= learning_rate * w4.grad\n",
        "\n",
        "    # zero gradients\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()\n",
        "    w3.grad.zero_()\n",
        "    w4.grad.zero_()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"epoch {epoch + 1}: \\nw1 = {w1}\\n w2 = {w2}\\n w3 = {w3}\\n w4 = {w4}, loss = {bce_loss:.8f}\")"
      ],
      "metadata": {
        "id": "1Z1QTEJVaQLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}